import os
import json
import random
import time
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from pydantic import BaseModel
import google.generativeai as genai
import re
from difflib import SequenceMatcher
import logging

# === CONFIGURACIÃ“N MEJORADA ===
def load_all_api_keys(path="src/tests/tokens.txt") -> List[str]:
    if not os.path.exists(path):
        print(f"âŒ Archivo no encontrado: {path}")
        exit(1)

    with open(path, "r", encoding="utf-8") as f:
        keys = [line.strip() for line in f if line.strip()]

    if not keys:
        print("âŒ El archivo de API keys estÃ¡ vacÃ­o")
        exit(1)

    print(f"âœ… {len(keys)} API keys cargadas desde {path}")
    return keys


# Cargar todas las API keys disponibles
ALL_API_KEYS = load_all_api_keys()
current_api_key_index = 0
API_KEY = ALL_API_KEYS[current_api_key_index]

# ConfiguraciÃ³n de directorios y lÃ­mites
DATA_DIR = Path("src/data")
DAILY_LIMIT = 500
RPM_LIMIT = 60  # Aumentado para permitir mÃ¡s peticiones
TPM_LIMIT = 250_000
REQUEST_LOG = "requests_today.txt"
STATS_FILE = "logs/stats_summary.json"
DETAIL_FILE = "logs/stats_details.json"
RETRY_LIMIT = 5
MAX_RUN_TIME = 30 * 60  # 30 minutos en segundos
TOKEN_BUDGET = 10000  # Presupuesto de tokens aumentado

# Inicializar directorios y archivos
os.makedirs("logs", exist_ok=True)

# Borrar y recrear archivos JSON vacÃ­os
for file_path in [STATS_FILE, DETAIL_FILE]:
    if os.path.exists(file_path):
        os.remove(file_path)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump({}, f, indent=2) if "summary" in file_path else json.dump([], f, indent=2)

# Configurar logging
logging.basicConfig(filename='logs/execution.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

# === MODELO PYDANTIC ===
class CocktailData(BaseModel):
    Url: str
    Name: str
    Glass: str
    Ingredients: List[str]
    Instructions: List[str]
    Review: str
    History: str
    Nutrition: str
    Alcohol_Content: List[List[str]]
    Garnish: str

# === GESTIÃ“N DE API KEYS ===
def switch_api_key():
    global current_api_key_index, API_KEY, requests_today, requests_this_minute
    current_api_key_index += 1
    
    if current_api_key_index >= len(ALL_API_KEYS):
        print("âš ï¸ No hay mÃ¡s API keys disponibles")
        return False
        
    API_KEY = ALL_API_KEYS[current_api_key_index]
    genai.configure(api_key=API_KEY)
    requests_today = 0
    requests_this_minute = 0
    print(f"ğŸ”„ Cambiando a API_KEY {current_api_key_index+1}")
    logging.info(f"Cambiando a API_KEY {current_api_key_index+1}")
    return True

# === CONTROL DE PETICIONES MEJORADO ===
requests_today = 0
if os.path.exists(REQUEST_LOG):
    with open(REQUEST_LOG) as f:
        requests_today = int(f.read().strip())

requests_this_minute = 0
last_minute = time.time()
tokens_used = 0

def wait_if_needed():
    global requests_this_minute, last_minute
    now = time.time()
    elapsed = now - last_minute
    
    # Reiniciar contador si ha pasado mÃ¡s de 1 minuto
    if elapsed >= 60:
        requests_this_minute = 0
        last_minute = now
    elif requests_this_minute >= RPM_LIMIT:
        sleep_time = 60 - elapsed
        print(f"â³ LÃ­mite RPM alcanzado. Esperando {sleep_time:.1f} segundos...")
        logging.warning(f"LÃ­mite RPM alcanzado. Esperando {sleep_time:.1f} segundos")
        time.sleep(sleep_time)
        requests_this_minute = 0
        last_minute = time.time()

def call_model(prompt: str) -> str:
    global requests_today, requests_this_minute, tokens_used
    
    retries = 0
    while retries < RETRY_LIMIT * 2:  # MÃ¡s reintentos
        # Verificar lÃ­mites antes de cada intento
        if requests_today >= DAILY_LIMIT:
            if not switch_api_key():
                return None
        
        wait_if_needed()
        
        try:
            response = gemini_model.generate_content(prompt)
            requests_this_minute += 1
            requests_today += 1
            
            # EstimaciÃ³n de tokens mÃ¡s precisa
            token_count = len(prompt.split()) + len(response.text.split())
            tokens_used += token_count
            
            # Actualizar archivo de contador
            with open(REQUEST_LOG, "w") as f:
                f.write(str(requests_today))
                
            return response.text
            
        except Exception as e:
            logging.error(f"Error en la API: {str(e)}")
            retries += 1
            wait_time = min(2 ** retries, 60)  # Espera exponencial con mÃ¡ximo 60 segundos
            print(f"â›” Error: {str(e)}. Reintentando en {wait_time} segundos...")
            time.sleep(wait_time)
    
    print("âš ï¸ Reintentos agotados para esta peticiÃ³n")
    logging.error("Reintentos agotados para peticiÃ³n")
    return None

# === FUNCIONES DE DATOS MEJORADAS ===
def get_non_empty_fields(data: CocktailData) -> List[Tuple[str, str]]:
    """Obtiene campos no vacÃ­os con sus valores convertidos a string"""
    fields = [
        ("Nombre", data.Name),
        ("Vaso", data.Glass),
        ("Ingredientes", data.Ingredients),
        ("Instrucciones", data.Instructions),
        ("ReseÃ±a", data.Review),
        ("Historia", data.History),
        ("NutriciÃ³n", data.Nutrition),
        ("Contenido_alcohÃ³lico", data.Alcohol_Content),
        ("DecoraciÃ³n", data.Garnish)
    ]
    
    non_empty = []
    for name, value in fields:
        if isinstance(value, list):
            # Manejar listas anidadas como Alcohol_Content
            if value and isinstance(value[0], list):
                flat_values = [f"{item[0]}: {item[1]}" for sublist in value for item in sublist]
                str_value = ". ".join(flat_values)
            else:
                str_value = ". ".join(map(str, value))
        elif isinstance(value, str):
            str_value = value.strip()
        else:
            str_value = str(value)
        
        if str_value:
            non_empty.append((name, str_value))
    
    return non_empty

def select_fields_for_stage(data: CocktailData, stage: int) -> Optional[List[Tuple[str, str]]]:
    """Selecciona campos para una etapa especÃ­fica, manejando campos vacÃ­os"""
    non_empty_fields = get_non_empty_fields(data)
    required_count = {
        1: 2, 2: 3, 3: 5, 4: 9, 5: 5, 6: 9
    }.get(stage, 2)
    
    # Para etapas que requieren todos los campos
    if stage in [4, 6]:
        if len(non_empty_fields) < required_count:
            logging.warning(f"Faltan campos para etapa {stage}: {len(non_empty_fields)}/{required_count}")
            return None
        return non_empty_fields
    
    # Para otras etapas, seleccionar aleatoriamente
    if len(non_empty_fields) < required_count:
        return None
    
    return random.sample(non_empty_fields, required_count)

def build_prompt_adaptive(stage: int, batch: List[Tuple[List[Tuple[str, str]], List[int]]]) -> str:
    """Builds the prompt with clear English instructions and examples"""
    stage_instructions = {
        1: "Generate a question based on ONE base fact that requires the target fact to answer.",
        2: "Generate a question based on ONE base fact that requires the TWO target facts to answer.",
        3: "Generate a question based on TWO base facts that requires the THREE target facts to answer.",
        4: "Generate a question based on THREE base facts that requires the SIX target facts to answer.",
        5: "Generate a question based on ONE base fact that requires the FOUR target facts to answer.",
        6: "Generate a question based on ONE base fact that requires the EIGHT target facts to answer."
    }
    
    header = f"""You are a cocktail expert. Your task is to generate questions about cocktails based on provided facts.

Instructions:
- Each entry contains base facts (to use in the question) and target facts (to answer)
- {stage_instructions[stage]}
- The question MUST require all target facts to be answered correctly
- DO NOT mention the target facts directly in the question
- The answer should be exactly the combined target facts
- Generate EXACTLY one question per entry, without numbering or prefixes

Examples:
* Stage 1:
  Base: Name: Mojito
  Target: Ingredients: Lime, mint, rum, sugar
  Question: What ingredients are needed to prepare a Mojito?

* Stage 3:
  Base: History: Created in 1997. Alcohol_Content: 1.3 standard drinks
  Target: Review: Jorge says it's good. Nutrition: 130 calories. Ingredients: Pineapple, chocolate, lemon
  Question: What are Jorge's reviews, the nutritional value and ingredients of a cocktail created in 1997 with 1.3 standard drinks of alcohol?

Now generate questions for the following entries:
"""
    
    body = []
    for i, (fields, base_indices) in enumerate(batch):
        base_facts = [f"{name}: {value}" for idx, (name, value) in enumerate(fields) if idx in base_indices]
        target_facts = [f"{name}: {value}" for idx, (name, value) in enumerate(fields) if idx not in base_indices]
        
        entry = f"Entry {i+1}:\n"
        entry += "Base facts:\n" + "\n".join(base_facts) + "\n"
        entry += "Target facts:\n" + "\n".join(target_facts)
        body.append(entry)
    
    return header + "\n\n".join(body) + "\n\nQuestions:\n"

def extract_questions(response: str) -> List[str]:
    """Extrae preguntas de la respuesta del modelo de forma robusta"""
    if not response:
        return []
    
    # Dividir por lÃ­neas y limpiar
    lines = [line.strip() for line in response.split('\n') if line.strip()]
    
    # Filtrar lÃ­neas que parecen preguntas
    questions = []
    for line in lines:
        # Ignorar encabezados y numeraciÃ³n
        if line.lower().startswith(("pregunta", "entrada", "1.", "2.", "3.", "4.", "5.", "-", "*")):
            continue
        
        # Considerar como pregunta si contiene signo de interrogaciÃ³n o tiene suficiente longitud
        if '?' in line or len(line) > 20:
            # Limpiar prefijos numÃ©ricos
            clean_line = re.sub(r'^\d+[\.\)]\s*', '', line)
            questions.append(clean_line)
    
    return questions

def generate_response(question: str, expected: str) -> str:
    """Genera una respuesta simulada con diferentes niveles de precisiÃ³n"""
    r = random.random()
    words = expected.split()
    
    if r < 0.6:
        return expected  # Respuesta correcta
    elif r < 0.85 and len(words) > 1:
        # Respuesta parcialmente correcta (falta algÃºn detalle)
        return " ".join(random.sample(words, max(1, len(words)//2)))
    else:
        # Respuesta incorrecta
        return "InformaciÃ³n no disponible" if random.random() < 0.5 else " ".join(random.sample(words, len(words)))

def get_response(question):
    pass

def get_true_correct(question: str, answer: str) -> dict:
    """
    EvalÃºa si una respuesta responde adecuadamente a una pregunta usando un modelo de lenguaje.
    
    Args:
        question (str): La pregunta original
        answer (str): La respuesta a evaluar
    
    Returns:
        dict: JSON con {'is_correct': bool} y detalles adicionales
    """
    # Construimos el prompt para el modelo evaluador
    prompt = f"""Eres un evaluador experto. Determina si la siguiente respuesta responde COMPLETAMENTE 
    y de manera CORRECTA a la pregunta. Considera:
    - La respuesta debe cubrir todos los aspectos de la pregunta
    - Debe ser factualmente correcta
    - No debe contener informaciÃ³n irrelevante

    Responde ÃšNICAMENTE con un JSON vÃ¡lido que contenga:
    - 'is_correct' (true/false)
    - 'reason' (breve explicaciÃ³n)

    Pregunta: {question}
    Respuesta: {answer}
    """
    
    try:
        model_response = gemini_model.generate_content(prompt)
        evaluation = model_response.text.strip().lower()
        
        if evaluation == 'true':
            return True
        elif evaluation == 'false':
            return False
        else:
            # Si el modelo no devuelve lo esperado, fallamos seguros
            logging.warning(f"Respuesta inesperada del modelo: {evaluation}")
            return False
            
    except Exception as e:
        logging.error(f"Error al consultar el modelo: {str(e)}")
        return False
    
def extract_json_from_response(text: str) -> dict:
    """
    Extrae un JSON de la respuesta del modelo, incluso si contiene texto alrededor.
    """
    # Busca el primer bloque que parezca JSON
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if not json_match:
        raise ValueError("No se encontrÃ³ JSON en la respuesta del modelo")
    
    try:
        return json.loads(json_match.group())
    except json.JSONDecodeError:
        raise ValueError("JSON invÃ¡lido en la respuesta del modelo")
    
def is_correct(expected: str, response: str) -> bool:
    """ComparaciÃ³n difusa de respuestas con tolerancia a errores menores"""
    if not expected or not response:
        return False
    
    # Normalizar textos
    expected_norm = re.sub(r'\W+', ' ', expected.lower()).strip()
    response_norm = re.sub(r'\W+', ' ', response.lower()).strip()
    
    # ComparaciÃ³n exacta
    if expected_norm == response_norm:
        return True
    
    # ComparaciÃ³n difusa
    ratio = SequenceMatcher(None, expected_norm, response_norm).ratio()
    return ratio > 0.7

def estimate_N_per_stage(stage: int) -> int:
    """Estima cuÃ¡ntas entradas procesar por etapa segÃºn tokens disponibles"""
    tokens_per_entry = {
        1: 150,  # 2 campos
        2: 200,  # 3 campos
        3: 300,  # 5 campos
        4: 500,  # 9 campos
        5: 300,  # 5 campos
        6: 500   # 9 campos
    }[stage]
    
    available = min(
        TOKEN_BUDGET // tokens_per_entry,
        DAILY_LIMIT - requests_today,
        RPM_LIMIT - requests_this_minute
    )
    return max(5, min(20, available))  # Entre 5 y 20 por lote

# === EJECUCIÃ“N DE ETAPAS CORREGIDA ===
def run_stage(stage: int, round_num: int) -> bool:
    print(f"\nğŸš€ Iniciando Etapa {stage} (Vuelta {round_num})")
    logging.info(f"Iniciando Etapa {stage} (Vuelta {round_num})")
    
    N = estimate_N_per_stage(stage)
    print(f"ğŸ“Š Estimado: {N} entradas para esta etapa")
    
    # Cargar datos
    files = list(DATA_DIR.glob("*.json"))
    if not files:
        print("âš ï¸ No se encontraron archivos JSON")
        return True
    
    selected_files = random.sample(files, min(N, len(files)))
    json_data = []
    for f in selected_files:
        try:
            with open(f, encoding="utf-8") as file:
                data = CocktailData(**json.load(file))
                json_data.append(data)
        except Exception as e:
            print(f"âš ï¸ Error en {f.name}: {str(e)}")
            logging.error(f"Error cargando {f.name}: {str(e)}")
    
    if not json_data:
        print("âš ï¸ No hay datos vÃ¡lidos para esta etapa")
        return True
    
    batch = []
    base_indices_config = {
        1: [0],  # 1 base de 2 campos
        2: [0],  # 1 base de 3 campos
        3: [0, 1],  # 2 bases de 5 campos
        4: [0, 1, 2],  # 3 bases de 9 campos
        5: [0],  # 1 base de 5 campos
        6: [0]   # 1 base de 9 campos
    }
    
    for data in json_data:
        fields = select_fields_for_stage(data, stage)
        if not fields:
            continue
        
        # Seleccionar Ã­ndices base
        base_indices = base_indices_config[stage]
        if stage in [1, 2, 5, 6]:
            # Para estas etapas, seleccionar un Ã­ndice base aleatorio
            base_indices = [random.randint(0, len(fields)-1)]
        elif stage == 3:
            # Seleccionar 2 Ã­ndices base aleatorios
            base_indices = random.sample(range(len(fields)), 2)
        elif stage == 4:
            # Seleccionar 3 Ã­ndices base aleatorios
            base_indices = random.sample(range(len(fields)), 3)
        
        batch.append((fields, base_indices))
    
    if not batch:
        print("âš ï¸ No se pudo preparar lote para esta etapa")
        return True
    
    try:
        prompt = build_prompt_adaptive(stage, batch)
        response_text = call_model(prompt)
        
        if response_text is None:
            print("â›” Error crÃ­tico en llamada al modelo")
            return False
            
        questions = extract_questions(response_text)
        print(f"ğŸ” ExtraÃ­das {len(questions)} preguntas de la respuesta")
    except Exception as e:
        print(f"â›” Error en el modelo: {str(e)}")
        logging.error(f"Error en modelo: {str(e)}")
        return True
    
    stats = {"OK": 0, "ERROR": 0}
    details = []
    
    for i, (fields, base_indices) in enumerate(batch):
        if i >= len(questions):
            print(f"âš ï¸ Faltan preguntas para la entrada {i+1}")
            continue
            
        q = questions[i]
        # Construir respuesta esperada con los campos objetivo
        expected_parts = []
        for idx, (name, value) in enumerate(fields):
            if idx not in base_indices:
                expected_parts.append(value)
        expected = ". ".join(expected_parts)
        
        if not expected:
            print(f"âš ï¸ Respuesta esperada vacÃ­a para entrada {i+1}")
            continue
            
        response = get_response(q)
        
        if get_true_correct(q, response):
            stats["OK"] += 1
            result_flag = "âœ…"
        else:
            stats["ERROR"] += 1
            result_flag = "âŒ"
        
        print(f"{result_flag} Q: {q[:70]}{'...' if len(q) > 70 else ''}")
        details.append({
            "stage": stage,
            "round": round_num,
            "question": q,
            "response": response,
            "expected": expected,
            "result": result_flag
        })
    
    # Guardar estadÃ­sticas
    try:
        with open(STATS_FILE, "r+", encoding="utf-8") as f:
            stats_data = json.load(f)
            stage_key = f"stage_{stage}"
            
            if stage_key not in stats_data:
                stats_data[stage_key] = {}
                
            round_key = f"round_{round_num}"
            if round_key not in stats_data[stage_key]:
                stats_data[stage_key][round_key] = stats
            else:
                # Actualizar estadÃ­sticas existentes
                existing = stats_data[stage_key][round_key]
                existing["OK"] += stats["OK"]
                existing["ERROR"] += stats["ERROR"]
            
            f.seek(0)
            json.dump(stats_data, f, ensure_ascii=False, indent=2)
            f.truncate()
    except Exception as e:
        print(f"â›” Error guardando estadÃ­sticas: {str(e)}")
        logging.error(f"Error guardando estadÃ­sticas: {str(e)}")
    
    # Guardar detalles
    try:
        with open(DETAIL_FILE, "r+", encoding="utf-8") as f:
            details_data = json.load(f)
            details_data.extend(details)
            f.seek(0)
            json.dump(details_data, f, ensure_ascii=False, indent=2)
            f.truncate()
    except Exception as e:
        print(f"â›” Error guardando detalles: {str(e)}")
        logging.error(f"Error guardando detalles: {str(e)}")
    
    print(f"ğŸ“Š Etapa {stage} completada: OK={stats['OK']}, ERROR={stats['ERROR']}")
    logging.info(f"Etapa {stage} completada: OK={stats['OK']}, ERROR={stats['ERROR']}")
    return True

# === FUNCIÃ“N PRINCIPAL MEJORADA ===
def main():
    start_time = time.time()
    stage_cycle = [1, 2, 3, 4, 5, 6]  # Orden de etapas
    stage_index = 0
    round_counter = 1
    requests_per_stage = {stage: 0 for stage in stage_cycle}
    stage_rounds = {stage: 0 for stage in stage_cycle}
    
    print(f"â±ï¸ Iniciando ejecuciÃ³n por {MAX_RUN_TIME/60} minutos")
    logging.info(f"Inicio ejecuciÃ³n. Tiempo mÃ¡ximo: {MAX_RUN_TIME/60} minutos")
    
    while (time.time() - start_time) < MAX_RUN_TIME:
        current_stage = stage_cycle[stage_index]
        stage_rounds[current_stage] = stage_rounds.get(current_stage, 0) + 1
        current_round = stage_rounds[current_stage]
        
        print(f"\nğŸ” Vuelta {round_counter} | Etapa {current_stage} | PeticiÃ³n {requests_per_stage.get(current_stage, 0)+1}/10")
        logging.info(f"Vuelta {round_counter} - Etapa {current_stage}")
        
        # Ejecutar etapa
        success = run_stage(current_stage, current_round)
        if not success:
            print("âš ï¸ Error crÃ­tico, deteniendo ejecuciÃ³n")
            logging.error("Error crÃ­tico, deteniendo ejecuciÃ³n")
            break
            
        requests_per_stage[current_stage] = requests_per_stage.get(current_stage, 0) + 1
        
        # Rotar etapa si se completaron 10 peticiones
        if requests_per_stage.get(current_stage, 0) >= 10:
            print(f"ğŸ¯ Completadas 10 peticiones para Etapa {current_stage}")
            requests_per_stage[current_stage] = 0
            stage_index = (stage_index + 1) % len(stage_cycle)
        
        round_counter += 1
        
        # Pausa breve entre iteraciones
        time.sleep(1)
    
    # Guardar estado final de peticiones
    with open(REQUEST_LOG, "w") as f:
        f.write(str(requests_today))
    
    elapsed = time.time() - start_time
    print(f"\nğŸ EjecuciÃ³n completada en {elapsed/60:.1f} minutos")
    print(f"ğŸ“Š Total peticiones: {requests_today}")
    print(f"ğŸ”‘ API keys usadas: {current_api_key_index+1}/{len(ALL_API_KEYS)}")
    logging.info(f"EjecuciÃ³n completada. Tiempo: {elapsed/60:.1f} min, Peticiones: {requests_today}, Keys usadas: {current_api_key_index+1}")

# === INICIALIZACIÃ“N ===
if __name__ == "__main__":
    # Configurar modelo Gemini
    genai.configure(api_key=API_KEY)
    gemini_model = genai.GenerativeModel("gemini-1.5-flash")
    
    print(f"ğŸ”‘ Usando API key {current_api_key_index+1}/{len(ALL_API_KEYS)}")
    print(f"â±ï¸ Tiempo mÃ¡ximo de ejecuciÃ³n: {MAX_RUN_TIME/60} minutos")
    print(f"ğŸ” Orden de etapas: [1, 2, 3, 4, 5, 6]")
    
    main()